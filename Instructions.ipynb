{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Usage\" data-toc-modified-id=\"Usage-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Usage</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Running-the-help-menu\" data-toc-modified-id=\"Running-the-help-menu-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>Running the help menu</a></span></li><li><span><a href=\"#Scraping-a-transcript\" data-toc-modified-id=\"Scraping-a-transcript-1.0.2\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;</span>Scraping a transcript</a></span><ul class=\"toc-item\"><li><span><a href=\"#Errors\" data-toc-modified-id=\"Errors-1.0.2.1\"><span class=\"toc-item-num\">1.0.2.1&nbsp;&nbsp;</span>Errors</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#For-Python-Developers\" data-toc-modified-id=\"For-Python-Developers-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>For Python Developers</a></span><ul class=\"toc-item\"><li><span><a href=\"#transcript_miner.py\" data-toc-modified-id=\"transcript_miner.py-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>transcript_miner.py</a></span><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Constants\" data-toc-modified-id=\"Constants-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Constants</a></span></li><li><span><a href=\"#valid_pdf()\" data-toc-modified-id=\"valid_pdf()-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span><code>valid_pdf()</code></a></span></li><li><span><a href=\"#define_college_sections()\" data-toc-modified-id=\"define_college_sections()-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span><code>define_college_sections()</code></a></span></li><li><span><a href=\"#find_college_sections_ends()\" data-toc-modified-id=\"find_college_sections_ends()-2.1.5\"><span class=\"toc-item-num\">2.1.5&nbsp;&nbsp;</span><code>find_college_sections_ends()</code></a></span></li><li><span><a href=\"#scrape_labels()\" data-toc-modified-id=\"scrape_labels()-2.1.6\"><span class=\"toc-item-num\">2.1.6&nbsp;&nbsp;</span><code>scrape_labels()</code></a></span></li><li><span><a href=\"#clean_labels\" data-toc-modified-id=\"clean_labels-2.1.7\"><span class=\"toc-item-num\">2.1.7&nbsp;&nbsp;</span><code>clean_labels</code></a></span></li><li><span><a href=\"#group_label_instances_by_college()\" data-toc-modified-id=\"group_label_instances_by_college()-2.1.8\"><span class=\"toc-item-num\">2.1.8&nbsp;&nbsp;</span><code>group_label_instances_by_college()</code></a></span></li><li><span><a href=\"#scrape_plans()\" data-toc-modified-id=\"scrape_plans()-2.1.9\"><span class=\"toc-item-num\">2.1.9&nbsp;&nbsp;</span><code>scrape_plans()</code></a></span></li><li><span><a href=\"#scrape_course_targets()\" data-toc-modified-id=\"scrape_course_targets()-2.1.10\"><span class=\"toc-item-num\">2.1.10&nbsp;&nbsp;</span><code>scrape_course_targets()</code></a></span></li><li><span><a href=\"#scrape_course_dept_and_seq()\" data-toc-modified-id=\"scrape_course_dept_and_seq()-2.1.11\"><span class=\"toc-item-num\">2.1.11&nbsp;&nbsp;</span><code>scrape_course_dept_and_seq()</code></a></span></li><li><span><a href=\"#scrape_targets()\" data-toc-modified-id=\"scrape_targets()-2.1.12\"><span class=\"toc-item-num\">2.1.12&nbsp;&nbsp;</span><code>scrape_targets()</code></a></span></li><li><span><a href=\"#gen_csv()\" data-toc-modified-id=\"gen_csv()-2.1.13\"><span class=\"toc-item-num\">2.1.13&nbsp;&nbsp;</span><code>gen_csv()</code></a></span></li></ul></li><li><span><a href=\"#main.py\" data-toc-modified-id=\"main.py-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>main.py</a></span><ul class=\"toc-item\"><li><span><a href=\"#scrape_transcript()\" data-toc-modified-id=\"scrape_transcript()-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span><code>scrape_transcript()</code></a></span></li><li><span><a href=\"#if-__name__-==-&quot;__main__&quot;\" data-toc-modified-id=\"if-__name__-==-&quot;__main__&quot;-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span><code>if __name__ == \"__main__\"</code></a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "   \n",
    "   __Los Rios CCD Transcript Extraction Tool__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the help menu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The executable version of the extractor has been compiled to c++ with the Nuitka compiler and will run at an order of magnitude faster than the python script. \n",
    "\n",
    "From a command line interface, call the executable `main.exe` (or the python script main.py) with the `--help` flag like so:\n",
    "\n",
    "    main.exe --help\n",
    "    \n",
    "This will show the following help message:\n",
    "    \n",
    "    usage: main.py [-h] PDF_IN CSV_OUT\n",
    "\n",
    "    Scrape Los Rios CCD Unofficial Transcripts\n",
    "\n",
    "    positional arguments:\n",
    "      PDF_IN      Path to transcript PDF to be scraped\n",
    "      CSV_OUT     path to CSV output file\n",
    "\n",
    "    optional arguments:\n",
    "      -h, --help  show this help message and exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping a transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the extractor with the infile (`PDF_IN`) and the outfile (`CSV_OUT`) as arguments/paramteres.\n",
    "\n",
    "    main.exe \"path/to/transcript.pdf\" \"path/to/extracted_data.csv\"\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "    \n",
    "##### Errors\n",
    "\n",
    "If there is an error or an invalid file is run, the script will write the error to `stdout` and __return__ `-1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Python Developers\n",
    "The executable file has been compiled to c/c++ using Nuitka compiler.\n",
    "\n",
    "\n",
    "The python environment depends on Python 3.6+ plus the following packages and their dependencies:\n",
    " - pandas\n",
    " - pdfquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transcript_miner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pdfquery as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAGEWIDTH = 792\n",
    "HALFPAGEWIDTH = 396\n",
    "PAGEHEIGHT = 612\n",
    "TOP_OF_PAGE = 523\n",
    "BOTTOM_OF_PAGE = 72"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `valid_pdf()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_pdf(pdf):\n",
    "    \"\"\"\n",
    "    Returns bool\n",
    "\n",
    "    Args:\n",
    "        pdf: loaded pdfquery pdf object\n",
    "    \"\"\"\n",
    "    pageheading = 'Los Rios CCD Unofficial Transcript - All'\n",
    "    pageheadings = pdf.pq('LTTextLineHorizontal:contains(\"%s\")' % pageheading)\n",
    "\n",
    "    valid_headings = [i[0].text.strip(\" \") == pageheading.strip(\" \")\n",
    "                      for i in pageheadings].count(True) == len(pdf.pq('LTFigure'))\n",
    "    valid_pagewidth = 792 == round(float(pdf.get_layout(1).bbox[2]))\n",
    "    valid_pageheight = 612 == round(float(pdf.get_layout(1).bbox[3]))\n",
    "\n",
    "    return all([valid_headings, valid_pagewidth, valid_pageheight])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `define_college_sections()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_college_sections(pdf, figures):\n",
    "    \"\"\"\n",
    "    Returns dict of Pandas DataFrames\n",
    "\n",
    "    Args:\n",
    "        pdf: loaded pdfquery pdf object\n",
    "        figures: list of LTFigure objects scraped from the PDF\n",
    "    \"\"\"\n",
    "\n",
    "    raw_scrape = {\"Beginning\": []}\n",
    "\n",
    "    for figure in figures:\n",
    "        pageid = float(figure.iterancestors('LTPage').__next__().layout.pageid)\n",
    "        for instance_wrapper in figure:\n",
    "            for instance in instance_wrapper.getchildren():\n",
    "                if \"----------Beginning\" in instance.text:\n",
    "                    raw_scrape[\"Beginning\"].append(dict(instance.attrib,\n",
    "                                                    **{\"pageid\":pageid,\n",
    "                                                        \"-y0\":-1 * float(instance.get('y0')),\n",
    "                                                        \"text\": instance.text,\n",
    "                                                        \"pageside\":\n",
    "                                                        float(instance.attrib['x0']) > HALFPAGEWIDTH\n",
    "                                                    }))\n",
    "\n",
    "        _instances = raw_scrape\n",
    "        instances = dict()\n",
    "\n",
    "        if len(_instances[\"Beginning\"]) > 0:\n",
    "            instances[\"Beginning\"] = \\\n",
    "            pd.DataFrame.from_records(_instances[\"Beginning\"]).apply( \\\n",
    "                                    lambda df_col: pd.to_numeric(df_col, \\\n",
    "                                        errors='ignore')).round(decimals=0)\n",
    "\n",
    "    return find_college_section_ends(instances, len(figures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `find_college_sections_ends()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_college_section_ends(instances, maxpageid):\n",
    "    \"\"\"\n",
    "    Adds a column to the instances[\"Beginning\"] dataframe which indicates\n",
    "    the last page of that college section of the given transcript.\n",
    "\n",
    "    Returns a dict of pandas dataframes.\n",
    "\n",
    "    Args:\n",
    "        instances: dict of pandas dataframes containng label instances\n",
    "        maxpageid: the page number of the final page of the transcript\n",
    "    \"\"\"\n",
    "    instances['Beginning']['last_pageid'] = 0\n",
    "    # find the pageid of the last page of each college section\n",
    "    for n, row in instances['Beginning'].iterrows():\n",
    "        if n < (len(instances['Beginning']) - 1):\n",
    "            instances[\"Beginning\"].at[n, 'last_pageid'] = instances['Beginning'].at[n+1, 'pageid'] - 1\n",
    "\n",
    "        elif n == (len(instances['Beginning']) - 1):\n",
    "            instances[\"Beginning\"].at[n, 'last_pageid'] = maxpageid\n",
    "\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `scrape_labels()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_labels(pdf, beginning_label_instances, figures):\n",
    "    \"\"\"\n",
    "    Returns a dict() containing key-value pairs for each label (text) that we\n",
    "    want to search for in the PDF. The values of the dict contain lists of\n",
    "    instances of each label and their respective meta-data (i.e. position on\n",
    "    page, pageid, etc.). The keys are the labels.\n",
    "\n",
    "    Args:\n",
    "        pdf: loaded pdfquery pdf object\n",
    "        figures: list of LTFigure objects scraped from the PDF\n",
    "    \"\"\"\n",
    "\n",
    "    semesters = (\"Spring\", \"Summer\", \"Fall\", \"Winter\",)\n",
    "    labels = (\"Plan\", \"Course\", \"Description\", \"Grade\", \"Attempted\", \"Earned\", \"Points\")\n",
    "\n",
    "    raw_scrape = dict()\n",
    "    for label in labels:\n",
    "        raw_scrape[label] = []\n",
    "    raw_scrape['semester'] = []\n",
    "\n",
    "    for figure in figures:\n",
    "        pageid = float(figure.iterancestors('LTPage').__next__().layout.pageid)\n",
    "\n",
    "        for label in labels+semesters:\n",
    "            for instance_wrapper in figure:\n",
    "                for instance in instance_wrapper.getchildren():\n",
    "                    if label in instance.text:\n",
    "                        if any([s in label for s in semesters]):\n",
    "                            raw_scrape['semester'].append(dict(instance.attrib,\n",
    "                                                          **{\"pageid\":pageid,\n",
    "                                                             \"-y0\":-1 * float(instance.get('y0')),\n",
    "                                                             \"text\": instance.text,\n",
    "                                                             \"pageside\":\n",
    "                                                             float(instance.attrib['x0']) > HALFPAGEWIDTH\n",
    "                                                            }))\n",
    "                        elif label == instance.text.strip(\" \").strip(\":\"):\n",
    "                            raw_scrape[label].append(dict(instance.attrib,\n",
    "                                                          **{\"pageid\":pageid,\n",
    "                                                             \"-y0\":-1 * float(instance.get('y0')),\n",
    "                                                             \"text\": instance.text,\n",
    "                                                             \"pageside\":\n",
    "                                                             float(instance.attrib['x0']) > HALFPAGEWIDTH\n",
    "                                                            }))\n",
    "\n",
    "        _instances = raw_scrape\n",
    "        instances = dict(beginning_label_instances)\n",
    "\n",
    "        for label in _instances.keys():\n",
    "            if len(_instances[label]) > 0:\n",
    "                instances[label] = \\\n",
    "                pd.DataFrame.from_records(_instances[label]).apply( \\\n",
    "                                        lambda df_col: pd.to_numeric(df_col, \\\n",
    "                                            errors='ignore')).round(decimals=0)\n",
    "\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `clean_labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_labels(instances):\n",
    "    \"\"\"\n",
    "    Cleans and sorts label instances.\n",
    "\n",
    "    Returns a dict of pandas dataframes.\n",
    "\n",
    "    Args:\n",
    "        instances: dict of pandas dataframes containng label instances\n",
    "    \"\"\"\n",
    "    # skip every third instance of these labels\n",
    "    skip_third = [\"Points\"]\n",
    "    # skip every second and third instance of these labels\n",
    "    skip_secondthird = [\"Attempted\", \"Earned\"]\n",
    "\n",
    "    # all labels\n",
    "    labels = [\"Beginning\", \"Plan\", \"Course\", \"Description\",\n",
    "              \"Grade\", \"Points\", \"Attempted\", \"Earned\", \"semester\"]\n",
    "\n",
    "    for s in skip_third:\n",
    "        df = instances[s]\n",
    "        instances[s] = df[(df.index+1) %3 != 0].reset_index(drop=True)\n",
    "\n",
    "    for s in skip_secondthird:\n",
    "        df = instances[s]\n",
    "        instances[s] = df[df.index %3 == 0].reset_index(drop=True)\n",
    "\n",
    "    instances['semester'] = instances['semester'].sort_values(by = [\"pageid\", \"pageside\", \"-y0\"]).reset_index(drop=True)\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `group_label_instances_by_college()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_label_instances_by_college(instances):\n",
    "    \"\"\"\n",
    "    Returns a dict of dicts of pandas dataframes.\n",
    "\n",
    "    Args:\n",
    "        instances: dict of pandas dataframes containng label instances\n",
    "    \"\"\"\n",
    "    colleges = dict()\n",
    "    labels = [\"Beginning\", \"Plan\", \"Course\", \"Description\",\n",
    "              \"Grade\", \"Points\", \"Attempted\", \"Earned\", \"semester\"]\n",
    "\n",
    "    for n, row in instances['Beginning'].iterrows():\n",
    "        collegename = row.text.split(\" \")[2]\n",
    "        colleges[collegename] = dict()\n",
    "        for label in labels:\n",
    "            df = instances[label]\n",
    "\n",
    "            colleges[collegename][label] = df[ (\n",
    "                                    (df['pageid'] >= row['pageid']) & \\\n",
    "                                    (df['pageid'] <= row['last_pageid'])\n",
    "                                ) &\n",
    "                                (\n",
    "                                    (\n",
    "                                        df['pageid'] > row['pageid']\n",
    "                                    ) |\n",
    "                                    (\n",
    "                                        (df['pageid'] == row['pageid']) &\n",
    "                                        (\n",
    "                                            (df['y0'] < row['y1']) |\n",
    "                                            (df['x0'] > HALFPAGEWIDTH)\n",
    "                                        )\n",
    "                                    )\n",
    "                                )].reset_index(drop=True)\n",
    "    return colleges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `scrape_plans()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_plans(pdf, colleges):\n",
    "    \"\"\"\n",
    "    Adds a new column called 'targets' to the 'Plan' dataframe.\n",
    "\n",
    "    Scrapes the names of the learning plans in each semester of the transcript into\n",
    "    this column.\n",
    "\n",
    "    Returns a dict of dicts of pandas dataframes.\n",
    "\n",
    "    Args:\n",
    "        pdf: loaded pdfquery pdf object\n",
    "        colleges: dict of dicts of dataframes containing label instances\n",
    "    \"\"\"\n",
    "    for college in colleges:\n",
    "        instances = colleges[college]\n",
    "        instances[\"Plan\"]['target'] = \"\"\n",
    "        for n, row in instances['Plan'].iterrows():\n",
    "            x0 = row['x1'] -1\n",
    "            y0 = row['y0'] -1\n",
    "            x1 = row['x1'] + 300\n",
    "            y1 = row['y1'] +1\n",
    "            raw_scrape = pdf.pq('LTTextLineHorizontal:in_bbox(\"%s, %s, %s, %s\")' % \\\n",
    "                                                                (x0, y0, x1, y1))\n",
    "            for i in raw_scrape:\n",
    "                if i.iterancestors('LTPage').__next__().layout.pageid == row['pageid']:\n",
    "                    target_text = ''.join([j.text for j in i])\n",
    "                    target = dict(i.attrib, **{\"text\": target_text,\n",
    "                                               \"pageid\": row['pageid']} )\n",
    "\n",
    "            instances[\"Plan\"].at[n, 'target'] = target\n",
    "            colleges[college][\"Plan\"] = instances[\"Plan\"]\n",
    "    return colleges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `scrape_course_targets()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_course_targets(pdf, colleges):\n",
    "    \"\"\"\n",
    "    Scrapes the department, sequence, description/title, attempted, earned\n",
    "    grade, and points for each course in each semester.\n",
    "\n",
    "    Args:\n",
    "        pdf: loaded pdfquery pdf object\n",
    "        colleges: dict of dicts of dataframes containing label instances\n",
    "    \"\"\"\n",
    "\n",
    "    for college in colleges:\n",
    "\n",
    "        instances = colleges[college] #type: pd.DataFrame\n",
    "        instances['Course']['targets'] = object\n",
    "\n",
    "        for n, row in instances['Course'].iterrows():\n",
    "            targets = {\n",
    "                        \"dept\":        [],\n",
    "                        \"seq\":         [],\n",
    "                        \"description\": [],\n",
    "                        \"attempted\":   [],\n",
    "                        \"earned\":      [],\n",
    "                        \"grade\":       [],\n",
    "                        \"points\":      []\n",
    "            }\n",
    "\n",
    "            # CASE: semester section starts on one page of the transcript and ends on the next\n",
    "            if instances[\"Points\"].loc[n*2+1]['pageid'] > row['pageid']:\n",
    "                # scrape part on first page\n",
    "                depts, seqs = \\\n",
    "                    scrape_course_dept_and_seq(\n",
    "                        pdf = pdf,\n",
    "                        pageid = row['pageid'],\n",
    "                        x0 = row['x0']-1,\n",
    "                        y0 = BOTTOM_OF_PAGE,\n",
    "                        x1 = instances['Description'].loc[n]['x0']+1,\n",
    "                        y1 = row['y0']+1,)\n",
    "                for dept, seq in zip(depts, seqs):\n",
    "                    targets['dept'].append(dept)\n",
    "                    targets['seq'].append(seq)\n",
    "                    targets_ = scrape_targets(pdf = pdf, x0 = 511, y0 = seq['y0'], y1 = seq['y1'], pageid = seq['pageid'])\n",
    "                    for key in targets_.keys():\n",
    "                        targets[key].append(targets_[key])\n",
    "\n",
    "                # scrape part on second page\n",
    "                depts, seqs = \\\n",
    "                    scrape_course_dept_and_seq(\n",
    "                        pdf = pdf,\n",
    "                        pageid = row['pageid'],\n",
    "                        x0 = 0,\n",
    "                        y0 = instances[\"Points\"].loc[n*2+1]['y1']-1,\n",
    "                        x1 = 162+1,\n",
    "                        y1 = TOP_OF_PAGE,)\n",
    "                for dept, seq in zip(depts, seqs):\n",
    "                    targets['dept'].append(dept)\n",
    "                    targets['seq'].append(seq)\n",
    "                    targets_ = scrape_targets(pdf = pdf, x0 = 161, y0 = seq['y0'], y1 = seq['y1'], pageid = seq['pageid'])\n",
    "                    for key in targets_.keys():\n",
    "                        targets[key].append(targets_[key])\n",
    "\n",
    "            # CASE: semester section starts in first column of page and ends in second column\n",
    "            elif instances[\"Points\"].loc[n*2+1]['pageside'] != row['pageside']:\n",
    "                # scrape part in first column\n",
    "                depts, seqs = \\\n",
    "                    scrape_course_dept_and_seq(\n",
    "                        pdf = pdf,\n",
    "                        pageid = row['pageid'],\n",
    "                        x0 = row['x0']-1,\n",
    "                        y0 = BOTTOM_OF_PAGE,\n",
    "                        x1 = instances['Description'].loc[n]['x0']+1,\n",
    "                        y1 = row['y0']+1,)\n",
    "                for dept, seq in zip(depts, seqs):\n",
    "                    targets['dept'].append(dept)\n",
    "                    targets['seq'].append(seq)\n",
    "                    targets_ = scrape_targets(pdf = pdf, x0 = 161, y0 = seq['y0'], y1 = seq['y1'], pageid = seq['pageid'])\n",
    "                    for key in targets_.keys():\n",
    "                        targets[key].append(targets_[key])\n",
    "\n",
    "                # scrape part in second column\n",
    "                depts, seqs = \\\n",
    "                    scrape_course_dept_and_seq(\n",
    "                        pdf = pdf,\n",
    "                        pageid = row['pageid'],\n",
    "                        x0 = HALFPAGEWIDTH,\n",
    "                        y0 = instances[\"Points\"].loc[n*2+1]['y1']-1,\n",
    "                        x1 = 512+1,\n",
    "                        y1 = TOP_OF_PAGE,)\n",
    "                for dept, seq in zip(depts, seqs):\n",
    "                    targets['dept'].append(dept)\n",
    "                    targets['seq'].append(seq)\n",
    "                    targets_ = scrape_targets(pdf = pdf, x0 = 511, y0 = seq['y0'], y1 = seq['y1'], pageid = seq['pageid'])\n",
    "                    for key in targets_.keys():\n",
    "                        targets[key].append(targets_[key])\n",
    "\n",
    "            # CASE: semester section starts and ends in same column on same page\n",
    "            else:\n",
    "                depts, seqs = \\\n",
    "                    scrape_course_dept_and_seq(\n",
    "                        pdf = pdf,\n",
    "                        pageid = row['pageid'],\n",
    "                        x0 = row['x0']-1,\n",
    "                        y0 = instances[\"Points\"].loc[n*2+1]['y1']-1,\n",
    "                        x1 = instances['Description'].loc[n]['x0']+1,\n",
    "                        y1 = row['y0']+1,)\n",
    "                for dept, seq in zip(depts, seqs):\n",
    "                    targets['dept'].append(dept)\n",
    "                    targets['seq'].append(seq)\n",
    "                    if row['pageside'] == False:\n",
    "                        x0 = 161\n",
    "                    else:\n",
    "                        x0 = 511\n",
    "                    targets_ = scrape_targets(pdf = pdf, x0 = x0, y0 = seq['y0'], y1 = seq['y1'], pageid = seq['pageid'])\n",
    "                    for key in targets_.keys():\n",
    "                        targets[key].append(targets_[key])\n",
    "\n",
    "            for label in targets.keys():\n",
    "                for target_instance in targets[label]:\n",
    "                    if target_instance != None:\n",
    "                        target_instance['label'] = label\n",
    "\n",
    "            instances['Course'].at[n, 'targets'] = list( zip( * [targets[key] for key in targets]))\n",
    "        colleges[college] = instances\n",
    "    return colleges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `scrape_course_dept_and_seq()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_course_dept_and_seq(pdf, pageid, x0, y0, x1, y1):\n",
    "    \"\"\"\n",
    "    For a particular semester / bbox area, returns two ordered lists of dicts:\n",
    "    course departments and course sequence numbers. Dicts contain the metadata\n",
    "    for each dept/seq.\n",
    "\n",
    "    Args:\n",
    "        pdf: loaded pdfquery pdf object\n",
    "        pageid: the page of the transcript to scrape\n",
    "        x0, y0, x1, y1: bbox coordinates to scrape from\n",
    "\n",
    "    \"\"\"\n",
    "    targets = []\n",
    "    raw_scrape = pdf.pq('LTTextLineHorizontal:in_bbox(\"%s, %s, %s, %s\")' % \\\n",
    "                                                        (x0, y0, x1, y1))\n",
    "    for i in raw_scrape:\n",
    "        if i.iterancestors('LTPage').__next__().layout.pageid == pageid:\n",
    "            if len(i) > 0:\n",
    "                for j in i:\n",
    "                    if j.text != None and j != None:\n",
    "                        targets.append(( dict(j.attrib, **{\"pageid\": pageid,\n",
    "                                                        \"text\": j.text.strip(\" \")}\n",
    "                                            )))\n",
    "            else:\n",
    "                if i.text != None and i != None:\n",
    "                    targets.append(( dict(i.attrib, **{\"pageid\": pageid,\n",
    "                                                    \"text\": i.text.strip(\" \")}\n",
    "                                        )))\n",
    "\n",
    "    depts = [i for i in targets if i['text'].isalpha()]\n",
    "    seqs = [i for i in targets if i['text'].isnumeric()]\n",
    "\n",
    "    return depts, seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `scrape_targets()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_targets(pdf, x0, y0, y1, pageid):\n",
    "    \"\"\"\n",
    "    Scrapes the description/title, attempted, earned, grade, and points for a given course.\n",
    "\n",
    "    Args:\n",
    "        pdf: loaded pdfquery pdf object\n",
    "        x0: initial x-position to begin iteratively scraping targets\n",
    "        y0: lower y-coordinate of scraped bbox\n",
    "        y1: upper y-coordinate of scraped bbox\n",
    "        pageid: the page of the transcript to scrape\n",
    "    \"\"\"\n",
    "    # generate pairs of (x0, x1)---the start and stop x-coordinates for each target instance\n",
    "    labels = [\"description\", \"attempted\", \"earned\", \"grade\", \"points\"]\n",
    "    xs_ = [float(x0)+i for i in [-1, 75, 103, 140, 169, 212]]\n",
    "    xs_ = list(zip(xs_, xs_[1:]))\n",
    "    xs = {labels[i]:xs_[i] for i in range(5)}\n",
    "    targets = {label:None for label in labels}\n",
    "    for label in xs.keys():\n",
    "        x0 = float(xs[label][0])-1\n",
    "        y0 = float(y0)-1\n",
    "        x1 = float(xs[label][1])+1\n",
    "        y1 = float(y1)+1\n",
    "        raw_scrape = pdf.pq('LTTextLineHorizontal:in_bbox(\"%s, %s, %s, %s\")' % \\\n",
    "                                                                (x0, y0, x1, y1))\n",
    "        for i in raw_scrape:\n",
    "            if i.iterancestors('LTPage').__next__().layout.pageid == pageid:\n",
    "                if len(i) > 0:\n",
    "                    for j in i:\n",
    "                        if j!=None and j.text!=None:\n",
    "                            targets[label] = dict(j.attrib, **{\"text\": j.text.strip(\" \"),\n",
    "                                                            \"pageid\": pageid})\n",
    "                else:\n",
    "                    if i!=None and i.text!=None:\n",
    "                        targets[label] = dict(i.attrib, **{\"text\": i.text.strip(\" \"),\n",
    "                                                        \"pageid\": pageid})\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `gen_csv()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_csv(pdf, colleges, filename_out):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        filename_out: name of csv file to output\n",
    "        pdf: loaded pdfquery pdf object\n",
    "        colleges: dict of dicts of dataframes containing label instances\n",
    "    \"\"\"\n",
    "    studentname = pdf.pq('LTTextLineHorizontal:contains(\"Name:\")')[0][0].text.split(\"Name: \")[1]\n",
    "    courses = []\n",
    "    for college in colleges:\n",
    "        for n, row in colleges[college]['Course'].iterrows():\n",
    "            course_targets = row['targets']\n",
    "            for target_set in course_targets:\n",
    "                courses.append(\n",
    "                    dict({(target['label'] if target!=None else \"missing\") : (target['text'] if target!=None else None) \\\n",
    "                         for target in target_set},\n",
    "                         **{\"semester\":colleges[college]['semester'].loc[n]['text'],\n",
    "                            \"plan\": colleges[college]['Plan'].loc[n]['target']['text'],\n",
    "                            \"college\": college,\n",
    "                            \"name\": studentname})\n",
    "                )\n",
    "\n",
    "    try:\n",
    "        pd.DataFrame(courses).drop('missing', 1).to_csv(filename_out, index=False)\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            pd.DataFrame(courses).to_csv(filename_out, index=False)\n",
    "        except:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `scrape_transcript()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_transcript(pdf_filepath, outfile_path):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pdf_filepath: path to transcript pdf to be scraped\n",
    "        outfile_path: path and filename of CSV outfile\n",
    "    \"\"\"\n",
    "    import transcript_miner as tm\n",
    "    import pdfquery as pq\n",
    "\n",
    "    is_good_pdf = False\n",
    "\n",
    "    try:\n",
    "        pdf = pq.PDFQuery(pdf_filepath)\n",
    "        pdf.load()\n",
    "\n",
    "        is_good_pdf = tm.valid_pdf(pdf)\n",
    "    except Exception as e:\n",
    "        print(\"Input file is invalid. Error: \", e)\n",
    "        return -1\n",
    "\n",
    "    if is_good_pdf:\n",
    "        figures = pdf.pq('LTFigure')\n",
    "\n",
    "        beginning_labels = tm.define_college_sections(pdf, figures)\n",
    "\n",
    "        label_instances = tm.scrape_labels(pdf, beginning_labels, figures)\n",
    "        label_instances = tm.clean_labels(label_instances)\n",
    "\n",
    "        colleges = tm.group_label_instances_by_college(label_instances)\n",
    "        colleges = tm.scrape_plans(pdf, colleges)\n",
    "        colleges = tm.scrape_course_targets(pdf, colleges)\n",
    "\n",
    "        tm.gen_csv(pdf, colleges, outfile_path)\n",
    "    else:\n",
    "        print(\"Invalid PDF.\")\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `if __name__ == \"__main__\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    import argparse\n",
    "\n",
    "    argparser = argparse.ArgumentParser(\\\n",
    "        description = \"Scrape Los Rios CCD Unofficial Transcripts\")\n",
    "\n",
    "    argparser.add_argument(\n",
    "        \"infile\", default = \"\",\n",
    "        help = \"Path to transcript PDF to be scraped\",\n",
    "        metavar = \"PDF_IN\"\n",
    "    )\n",
    "\n",
    "    argparser.add_argument(\n",
    "        \"outfile\", default = \"\",\n",
    "        help = \"path to CSV output file\",\n",
    "        metavar = \"CSV_OUT\"\n",
    "    )\n",
    "\n",
    "    args = argparser.parse_args()\n",
    "\n",
    "    scrape_transcript(pdf_filepath = args.infile,\n",
    "                      outfile_path = args.outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
